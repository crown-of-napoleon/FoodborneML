{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Official Results for 'Discovering Foodborne Illness in Online Restaurant Reviews' in JAMIA, 2017\n",
    "\n",
    "This notebook presents the official evaluation of classifiers presented in the paper.\n",
    "\n",
    "Because of licensing restrictions, we cannot publish the data used in the experiments.\n",
    "\n",
    "However, the final trained models may be found in the `best_models` directory for inspection and further experimentation.\n",
    "\n",
    "Additionally, those models are all retrained on the full training data within this notebook with a fixed random seed so that the final numbers could be reproduced if needed.\n",
    "\n",
    "For inqueries about the results, models, or data, please contact [Tom Effland](mailto:teffland@cs.columbia.edu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Derivations of Selection Bias Corrections](#Selection-Bias-Correction)\n",
    "    1. [Training](#Training:-Error-Rate)\n",
    "    2. [Testing](#)\n",
    "    \n",
    "2. [Setup](#)\n",
    "    1. [Data Ingestion](#) (Data is cleaned ahead of time)\n",
    "    4. [Loading Models](#) (Model hyperparams are pre-tuned)\n",
    "3. [Sick Task](#)\n",
    "    1. [Logistic Regression](#)\n",
    "    2. [Random Forest](#)\n",
    "    3. [SVM](#)\n",
    "    4. [Prototype](#)\n",
    "4. [Multiple Task](#)\n",
    "    1. [Logistic Regression](#)\n",
    "    2. [Random Forest](#)\n",
    "    3. [SVM](#)\n",
    "    4. [Pipelined Logistic Regression](#)\n",
    "    5. [Prototype](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Bias Correction\n",
    "\n",
    "Let $T(x)$ be the biased selection process for labelling data at DOHMH. We treat it as a black-box and model it atomically.\n",
    "\n",
    "Let $U$ be the set of all Yelp Reviews that have been processed by the system.\n",
    "\n",
    "Let $B \\subset U$ s.t. $B = \\{(x,y) | T(x) = 1\\}$. $B$ is the biased training set.\n",
    "\n",
    "Let $B^c \\subseteq U \\setminus B = \\{(x,y) | T(x) = 0\\}$. $B^c$ is the biased complement training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Error Rate\n",
    "\n",
    "We can model the error rate of some classifier $f$ as:\n",
    "\n",
    "$p(f(x) \\ne y) = p(f(x) \\ne y | T(x) = 1)p(T(x) = 1) + p(f(x) \\ne y | T(x) = 0)p(T(x) = 0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use plugin estimates:\n",
    "\n",
    "$\\hat{p}(T(x) = 1) = \\frac{|B|}{|U|}$ (the % of labeled points)\n",
    "\n",
    "$\\hat{p}(f(x) \\neq y | T(x)=1) = \\frac{1}{|B|}\\Sigma_{(x,y) \\in B}{I[f(x) \\neq y]}$\n",
    "\n",
    "likewise,\n",
    "\n",
    "$\\hat{p}(f(x) \\neq y | T(x)=0) = \\frac{1}{|B^c|}\\Sigma_{(x,y) \\in B^c}{I[f(x) \\neq y]}$\n",
    "\n",
    "therefore\n",
    "\n",
    "$\\hat{p}(f(x) \\neq y) = w_B\\Sigma_{(x,y) \\in B}{I[f(x) \\neq y]} + w_{B^c}\\Sigma_{(x,y) \\in B^c}{I[f(x) \\neq y]}$\n",
    "\n",
    "where \n",
    "\n",
    "$w_B = \\frac{1}{|U|}$ and $w_{B^c} = (1 - \\frac{|B|}{|U|})(\\frac{1}{|B^c|})$\n",
    "\n",
    "Note that when $B^c$ is the entire complement of $B$, then $w_{B^c}$ reduces to $\\frac{1}{|U|}$\n",
    "\n",
    "So, if we are trying to minimize the classification error in the objective function then these are the sample weights to be used when calculating the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "We will evaluate models using the F1 score and AUPR metric, calculated empirically on the dev and test sets.\n",
    "\n",
    "We can bias-correct these the same way, using bias corrections for the precision and recall. Consider the probabilistic form of precision for some classifier $f$:\n",
    "\n",
    "#### Precision\n",
    "\n",
    "Let $U$ be the test set.\n",
    "\n",
    "$Precision = p(y=1|f(x)=1) = \\Sigma_{t}{p(y=1 | f(x)=1, T(x) = t)p(T(x)=t|f(x)=1)}$\n",
    "\n",
    "Let $U_P \\subset U$ be the set $\\{(x,y) | f(x) = 1\\}$. \n",
    "\n",
    "**Note** that $U_P$ varies with the classification threshold.\n",
    "\n",
    "Then as with the error rate, we can find plugin estimates for the necessary quanities.\n",
    "\n",
    "$\\hat{p}(T(x) = 1|f(x)=1) = \\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B}{I[f(x)=1]} = \\frac{|B \\cap U_P|}{|U_P|}$\n",
    "\n",
    "and so $\\hat{p}(T(x) = 0|f(x)=1) = 1 - \\frac{|B \\cap U_P|}{|U_P|}$\n",
    "\n",
    "Also,\n",
    "\n",
    "$\\hat{p}(y=1|f(x)=1, T(x)=1) = \\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B\\cap U_P}{I[y=1]} \n",
    " $\n",
    "\n",
    "and $\\hat{p}(y=1|f(x)=1, T(x)=0) = \\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B^c\\cap U_P}{I[y=1]}$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$Precision = \\frac{|B \\cap U_P|}{|U_P|} \\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B\\cap U_P}{I[y=1]} \n",
    "+ (1 - \\frac{|B \\cap U_P|}{|U_P|})\\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B^c\\cap U_P}{I[y=1]}$\n",
    "\n",
    "#### Recall\n",
    "\n",
    "The derivation for improtance-weighted (IW) recall follows similary to IW precision.\n",
    "\n",
    "$Recall = p(f(x)=1|y=1) = \\Sigma_{t}{p(f(x)=1| y=1, T(x) = t)p(T(x)=t|y=1)}$\n",
    "\n",
    "Let $U_R \\subset U$ be the set $\\{(x,y) | y = 1\\}$. \n",
    "\n",
    "**Note** that $U_R$ **does not** vary with the classification threshold.\n",
    "\n",
    "The plugin estimates are:\n",
    "\n",
    "$\\hat{p}(T(x) = 1|y=1) = \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B}{I[y=1]} = \\frac{|B \\cap U_R|}{|U_R|}$\n",
    "\n",
    "and so $\\hat{p}(T(x) = 0|y=1) = 1 - \\frac{|B \\cap U_R|}{|U_R|}$\n",
    "\n",
    "Also,\n",
    "\n",
    "$\\hat{p}(f(x)=1 |y=1, T(x)=1) = \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B\\cap U_R}{I[f(x)=1]}$\n",
    "\n",
    "and $\\hat{p}(f(x)=1|y=1, T(x)=0) = \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B^c\\cap U_R}{I[f(x)=1]}$\n",
    "\n",
    "plugging in we have\n",
    "\n",
    "$ Recall =  \\frac{|B \\cap U_R|}{|U_R|} \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B\\cap U_R}{I[f(x)=1]}\n",
    "+ (1 - \\frac{|B \\cap U_R|}{|U_R|}) \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B^c\\cap U_R}{I[f(x)=1]}$\n",
    "\n",
    "#### AUPR Curve\n",
    "\n",
    "Finally, using the above plugin estimates, we can balance data from both $B$ and (a sample from) $B^c$.  We can obtain a series of ordered Precision-Recall points $E = \\{(p,r,t)_i | r_i \\leq r_{i'}\\}$ by varying the classification threshold $t \\in [0,1]$ and then using trapezoidal integration to approximate the area under the Recall vs. Precision curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrap\n",
    "\n",
    "For the final evaluation, we would like confidence intervals about the F1 and AUPR. We find these by using the percentile bootstrap:\n",
    "\n",
    "We calculate bootstrap statistics for the IW-Precision, IW-Recall, and IW-AUPR as follows:\n",
    "\n",
    "First we calculate the statistic $\\bar{x}$ (for each of IW-Precision, IW-Recall, and IW-AUPR).\n",
    "\n",
    "Then we resample the test dataset with replacement $B$ times and obtain the bootstrap statisic estimates for each set.\n",
    "\n",
    "Call these $x_1, ...,  x_B$.\n",
    "\n",
    "Then we can compute confidence intervals around $\\bar{x}$ the usual way by finding the $\\alpha=.025$ boundary quantiles $\\delta_{\\alpha}, \\delta_{1-\\alpha}$, such that \n",
    "\n",
    "$$P(\\bar{x}^* - \\delta_{1-\\alpha} \\leq \\bar{x} \\leq \\bar{x}^* - \\delta_{\\alpha}) = .95$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "%matplotlib inline\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.baseline_experiment_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_data = setup_baseline_data(data_path='data', train_regime='biased', test_regime='gold', random_seed=random_seed)\n",
    "gold_data = setup_baseline_data(data_path='data', train_regime='gold', test_regime='gold', random_seed=random_seed)\n",
    "silver_data = setup_baseline_data(data_path='data', train_regime='silver', test_regime='gold', random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train setup\n",
    "biased_text = np.array(biased_data['train_data']['text'])\n",
    "biased_sick = np.array(biased_data['train_data']['is_foodborne'])\n",
    "biased_mult = np.array(biased_data['train_data']['is_multiple'])\n",
    "biased_weights = calc_train_importance_weights(biased_data['train_data']['is_biased'], biased_data['U'])\n",
    "\n",
    "gold_text = np.array(gold_data['train_data']['text'])\n",
    "gold_sick = np.array(gold_data['train_data']['is_foodborne'])\n",
    "gold_mult = np.array(gold_data['train_data']['is_multiple'])\n",
    "gold_weights = calc_train_importance_weights(gold_data['train_data']['is_biased'], gold_data['U'])\n",
    "\n",
    "silver_text = np.array(silver_data['train_data']['text'])\n",
    "silver_sick = np.array(silver_data['train_data']['is_foodborne'])\n",
    "silver_mult = np.array(silver_data['train_data']['is_multiple'])\n",
    "silver_weights = calc_train_importance_weights(silver_data['train_data']['is_biased'], silver_data['U'])\n",
    "\n",
    "# test setup\n",
    "test_data = gold_data['test_data']\n",
    "B = 1000 # number of bootstrap test set resamples\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.lr_model import model as lr_model\n",
    "from experiments.rf_model import model as rf_model\n",
    "from experiments.svm_model import model as svm_model\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression sick models\n",
    "best_sick_lr_biased = joblib.load('experiments/best_models/best_lr_sick_biased.pkl')\n",
    "best_sick_lr_gold = joblib.load('experiments/best_models/best_lr_sick_gold.pkl')\n",
    "best_sick_lr_silver = joblib.load('experiments/best_models/best_lr_sick_silver.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest sick models\n",
    "best_sick_rf_biased = joblib.load('experiments/best_models/best_rf_sick_biased.pkl')\n",
    "best_sick_rf_gold = joblib.load('experiments/best_models/best_rf_sick_gold.pkl')\n",
    "best_sick_rf_silver = joblib.load('experiments/best_models/best_rf_sick_silver.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm sick models\n",
    "best_sick_svm_biased = joblib.load('experiments/best_models/best_svm_sick_biased.pkl')\n",
    "best_sick_svm_gold = joblib.load('experiments/best_models/best_svm_sick_gold.pkl')\n",
    "best_sick_svm_silver = joblib.load('experiments/best_models/best_svm_sick_silver.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression multiple models\n",
    "best_mult_lr_biased = joblib.load('experiments/best_models/best_lr_mult_biased.pkl')\n",
    "best_mult_lr_gold = joblib.load('experiments/best_models/best_lr_mult_gold.pkl')\n",
    "best_mult_lr_silver = joblib.load('experiments/best_models/best_lr_mult_silver.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest multiple models\n",
    "best_mult_rf_biased = joblib.load('experiments/best_models/best_rf_mult_biased.pkl')\n",
    "best_mult_rf_gold = joblib.load('experiments/best_models/best_rf_mult_gold.pkl')\n",
    "best_mult_rf_silver = joblib.load('experiments/best_models/best_rf_mult_silver.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm multiple models\n",
    "best_mult_svm_biased = joblib.load('experiments/best_models/best_svm_mult_biased.pkl')\n",
    "best_mult_svm_gold = joblib.load('experiments/best_models/best_svm_mult_gold.pkl')\n",
    "best_mult_svm_silver = joblib.load('experiments/best_models/best_svm_mult_silver.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sick Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the notable stats for testing on the Sick task:\n",
    "* All the test data is from 1/1/2017 and later\n",
    "* It's about 2/3 biased and 1/3 nonbiased (1975 and 1000 reviews, respectively)\n",
    "* All 1000 nonbiased reviews are have `No` labels\n",
    "* The 1975 biased reviews are about 52% `Yes`/`No` (1026/949)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sick_lr_biased.fit(biased_text, biased_sick, logreg__sample_weight=biased_weights)\n",
    "best_sick_lr_gold.fit(gold_text, gold_sick, logreg__sample_weight=gold_weights)\n",
    "best_sick_lr_silver.fit(silver_text, silver_sick, logreg__sample_weight=silver_weights)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Logistic Regression Sick Biased'\n",
    "all_results[title] = model_report(best_sick_lr_biased, title, 'is_foodborne',\n",
    "                                  test_data=test_data,\n",
    "                                  save_fname='figures/sick_lr_biased',\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds for evaluation'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Logistic Regression Sick Gold'\n",
    "all_results[title] = model_report(best_sick_lr_gold, title, 'is_foodborne', \n",
    "                                  test_data=test_data,\n",
    "                                   save_fname='figures/sick_lr_gold',\n",
    "                                   B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Logistic Regression Sick Silver'\n",
    "all_results[title] = model_report(best_sick_lr_silver, title, 'is_foodborne', \n",
    "                                  test_data=test_data,\n",
    "                                  save_fname='figures/sick_lr_silver',\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision Recall Tradeoffs\n",
    "\n",
    "We'd like to explore how we lose precision with the LR models, as we gain recall. This can be visualized by looking at the high recall region of the PR curves. \n",
    "\n",
    "In the curve we can see that all of the model precision begins to drop around a recall of .8 start to significantly drop precision around a recall of .9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_curves([best_sick_lr_biased, best_sick_lr_gold, best_sick_lr_silver], \n",
    "          ['\"Sick\" Logistic Regression Biased', \n",
    "           '\"Sick\" Logistic Regression Gold', \n",
    "           '\"Sick\" Logistic Regression Silver'], \n",
    "          'Precision-Recall Tradeoffs', 'is_foodborne', \n",
    "          dashes=[[20,5], [10,3], [5,1]],\n",
    "          test_data=test_data, save_fname='paper_sick_lr',\n",
    "          figsize=(6,4),\n",
    "          xlim=(.5,1.),\n",
    "          yticks=.1*np.arange(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_model_hyperparams(best_sick_lr_biased, 'Logistic Regression Sick Biased')\n",
    "print\n",
    "print_model_hyperparams(best_sick_lr_gold, 'Logistic Regression Sick Gold')\n",
    "print\n",
    "print_model_hyperparams(best_sick_lr_silver, 'Logistic Regression Sick Silver')\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sick_rf_biased.fit(biased_text, biased_sick, rf__sample_weight=biased_weights)\n",
    "best_sick_rf_gold.fit(gold_text, gold_sick, rf__sample_weight=gold_weights)\n",
    "best_sick_rf_silver.fit(silver_text, silver_sick, rf__sample_weight=silver_weights)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Random Forest Sick Biased'\n",
    "all_results[title] = model_report(best_sick_rf_biased, title, 'is_foodborne',\n",
    "                                  save_fname='figures/sick_rf_biased',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Random Forest Sick Gold'\n",
    "all_results[title] = model_report(best_sick_rf_gold, title, 'is_foodborne',\n",
    "                                  save_fname='figures/sick_rf_gold',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Random Forest Sick Silver'\n",
    "all_results[title] = model_report(best_sick_rf_silver, title, 'is_foodborne',\n",
    "                                  save_fname='figures/sick_rf_silver',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_model_hyperparams(best_sick_rf_biased, 'Random Forest Sick Biased')\n",
    "print\n",
    "print_model_hyperparams(best_sick_rf_gold, 'Random Forest Sick Gold')\n",
    "print\n",
    "print_model_hyperparams(best_sick_rf_silver, 'Random Forest Sick Silver')\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sick_svm_biased.fit(biased_text, biased_sick, svc__sample_weight=biased_weights)\n",
    "best_sick_svm_gold.fit(gold_text, gold_sick, svc__sample_weight=gold_weights)\n",
    "best_sick_svm_silver.fit(silver_text, silver_sick, svc__sample_weight=silver_weights)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'SVM Sick Biased'\n",
    "all_results[title] = model_report(best_sick_svm_biased, title, 'is_foodborne',\n",
    "                                  save_fname='figures/sick_svm_biased',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'SVM Sick Gold'\n",
    "all_results[title] = model_report(best_sick_svm_gold, title, 'is_foodborne',\n",
    "                                  save_fname='figures/sick_svm_gold',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'SVM Sick Silver'\n",
    "all_results[title] = model_report(best_sick_svm_silver, title, 'is_foodborne',\n",
    "                                  save_fname='figures/sick_svm_silver',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_model_hyperparams(best_sick_svm_biased, 'SVM Sick Biased')\n",
    "print\n",
    "print_model_hyperparams(best_sick_svm_gold, 'SVM Sick Gold')\n",
    "print\n",
    "print_model_hyperparams(best_sick_svm_silver, 'SVM Sick Silver')\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sick Prototype\n",
    "\n",
    "The prototype classifier is build in the Weka library in Java. To evaluate this, we ran the model on the same test data in the java environment and output the scores to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_sick_scores = pd.read_csv('data/prototype_sick_scores.csv')\n",
    "prototype_sick_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_sick_results = prototype_model_report(test_data['is_foodborne'], \n",
    "                                    prototype_sick_scores['0'].tolist(), \n",
    "                                    test_data['is_biased'], 'J4.8', \n",
    "                                    B=B, random_seed=random_seed)\n",
    "all_results['Prototype Sick'] = prototype_sick_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the notable stats for testing on the Multple task:\n",
    "* All the test data is from 1/1/2017 and later\n",
    "* It's about 2/3 biased and 1/3 nonbiased (1975 and 1000 reviews, respectively)\n",
    "* All 1000 nonbiased reviews all have `No` labels\n",
    "* The 1975 biased reviews are about 14% `Yes` (277 `Yes`, 1698 `No`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mult_lr_biased.fit(biased_text, biased_mult, logreg__sample_weight=biased_weights)\n",
    "best_mult_lr_gold.fit(gold_text, gold_mult, logreg__sample_weight=gold_weights)\n",
    "best_mult_lr_silver.fit(silver_text, silver_mult, logreg__sample_weight=silver_weights)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Logistic Regression Mult Biased'\n",
    "all_results[title] = model_report(best_mult_lr_biased, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_lr_biased',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Logistic Regression Mult Gold'\n",
    "all_results[title] = model_report(best_mult_lr_gold, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_lr_gold',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Logistic Regression Mult Silver'\n",
    "all_results[title] = model_report(best_mult_lr_silver, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_lr_silver',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_model_hyperparams(best_mult_lr_biased, 'Logistic Regression Mult Biased')\n",
    "print\n",
    "print_model_hyperparams(best_mult_lr_gold, 'Logistic Regression Mult Gold')\n",
    "print\n",
    "print_model_hyperparams(best_mult_lr_silver, 'Logistic Regression Mult Silver')\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelined Logistic Regression\n",
    "\n",
    "Since the Multiple is always conditioned on Sick=Yes (by definition), it makes more sense to train and use the Multuple model only on data where Sick=Yes. This may allow the model to focus better on Multiple-related features and not need to also learn the Sick class.\n",
    "\n",
    "We try this technique for all three training regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultPipeline():\n",
    "    def __init__(self, sick_model, mult_model, sick_threshold=.5):\n",
    "        self.sick_model = sick_model\n",
    "        self.mult_model = mult_model\n",
    "        self.t = sick_threshold\n",
    "    \n",
    "    def predict_proba(self, xs):\n",
    "        sick_probs = self.sick_model.predict_proba(xs)\n",
    "        not_sick = sick_probs[:,1] < self.t\n",
    "        mult_probs = self.mult_model.predict_proba(xs)\n",
    "        mult_probs[not_sick] = sick_probs[not_sick]\n",
    "        return mult_probs\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        return (self.predict_proba(xs)[:,1] >= self.t).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_only = biased_sick.astype(np.bool)\n",
    "sick_only_text = biased_text[sick_only]\n",
    "sick_only_mult = biased_mult[sick_only]\n",
    "sick_only_weights = biased_weights[sick_only]\n",
    "print \"{} training data / {} orginally\".format(len(sick_only_text), len(biased_text))\n",
    "print \"{0:2.2f}% yes, {1:2.2f}% biased\".format(\n",
    "    100.*sick_only_mult.sum()/len(sick_only_mult), \n",
    "    100.*np.array(biased_data['train_data']['is_biased'])[sick_only].sum()/len(sick_only_mult), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_only_mult_lr_biased = deepcopy(best_mult_lr_biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_only_mult_lr_biased.fit(sick_only_text, sick_only_mult, logreg__sample_weight=sick_only_weights)\n",
    "sick_only_mult_model_biased = MultPipeline(best_sick_lr_biased, sick_only_mult_lr_biased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Logistic Regression - Sick Only Biased Multiple'\n",
    "all_results[title] = model_report(sick_only_mult_model_biased, title, 'is_multiple', \n",
    "                                  B=B, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_only_mult_lr_gold = deepcopy(best_mult_lr_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_only_mult_lr_gold.fit(sick_only_text, sick_only_mult, logreg__sample_weight=sick_only_weights)\n",
    "sick_only_mult_model_gold = MultPipeline(best_sick_lr_gold, sick_only_mult_lr_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Logistic Regression - Sick Only Gold Multiple'\n",
    "all_results[title] = model_report(sick_only_mult_model_gold, title, 'is_multiple', \n",
    "                                  B=B, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_only_mult_lr_silver = deepcopy(best_mult_lr_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_only_mult_lr_silver.fit(sick_only_text, sick_only_mult, logreg__sample_weight=sick_only_weights)\n",
    "sick_only_mult_model_silver = MultPipeline(best_sick_lr_silver, sick_only_mult_lr_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Logistic Regression - Sick Only Silver Multiple'\n",
    "all_results[title] = model_report(sick_only_mult_model_silver, title, 'is_multiple', \n",
    "                                  B=B, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mult_rf_biased.fit(biased_text, biased_mult, rf__sample_weight=biased_weights)\n",
    "best_mult_rf_gold.fit(gold_text, gold_mult, rf__sample_weight=gold_weights)\n",
    "best_mult_rf_silver.fit(silver_text, silver_mult, rf__sample_weight=silver_weights)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Random Forest Mult Biased'\n",
    "all_results[title] = model_report(best_mult_rf_biased, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_rf_biased',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Random Forest Mult Gold'\n",
    "all_results[title] = model_report(best_mult_rf_gold, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_rf_gold',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'Random Forest Mult Silver'\n",
    "all_results[title] = model_report(best_mult_rf_silver, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_rf_silver',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_model_hyperparams(best_mult_rf_biased, 'Random Forest Mult Biased')\n",
    "print\n",
    "print_model_hyperparams(best_mult_rf_gold, 'Random Forest Mult Gold')\n",
    "print\n",
    "print_model_hyperparams(best_mult_rf_silver, 'Random Forest Mult Silver')\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mult_svm_biased.fit(biased_text, biased_mult, svc__sample_weight=biased_weights)\n",
    "best_mult_svm_gold.fit(gold_text, gold_mult, svc__sample_weight=gold_weights)\n",
    "best_mult_svm_silver.fit(silver_text, silver_mult, svc__sample_weight=silver_weights)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'SVM Mult Biased'\n",
    "all_results[title] = model_report(best_mult_svm_biased, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_svm_biased',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'SVM Mult Gold'\n",
    "all_results[title] = model_report(best_mult_svm_gold, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_svm_gold',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "title = 'SVM Mult Silver'\n",
    "all_results[title] = model_report(best_mult_svm_silver, title, 'is_multiple',\n",
    "                                  save_fname='figures/mult_svm_silver',\n",
    "                                  test_data=test_data,\n",
    "                                  B=B, random_seed=random_seed)\n",
    "print '\\n{} seconds'.format(int(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_model_hyperparams(best_mult_svm_biased, 'SVM Mult Biased')\n",
    "print\n",
    "print_model_hyperparams(best_mult_svm_gold, 'SVM Mult Gold')\n",
    "print\n",
    "print_model_hyperparams(best_mult_svm_silver, 'SVM Mult Silver')\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Prototype\n",
    "\n",
    "The prototype classifier is build in the Weka library in Java. To evaluate this, we ran the model on the same test data in the java environment and output the scores to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_mult_scores = pd.read_csv('data/prototype_mult_scores.csv')\n",
    "prototype_mult_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype_mult_results = prototype_model_report(test_data['is_multiple'], \n",
    "                                    prototype_mult_scores['0'].tolist(), \n",
    "                                    test_data['is_biased'], 'J4.8', \n",
    "                                    B=B, random_seed=random_seed)\n",
    "all_results['Prototype Multiple'] = prototype_mult_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the scores into a nice table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_table = pd.DataFrame()\n",
    "mult_table = pd.DataFrame()\n",
    "for name, result in sorted(all_results.items(), key=lambda x:x[0]):\n",
    "    data = {k:v for k,v in result.items() if ('samples' not in k) and ('_ci' not in k) }\n",
    "    data.update({k+'_b':v[0] for k,v in result.items() if '_ci' in k})\n",
    "    data.update({k+'_t':v[1] for k,v in result.items() if '_ci' in k})\n",
    "    data['name'] = name\n",
    "    if 'Sick' in name:\n",
    "        sick_table = sick_table.append(data, ignore_index=True)\n",
    "    else:\n",
    "        mult_table = mult_table.append(data, ignore_index=True)\n",
    "sick_table.set_index('name', inplace=True)\n",
    "mult_table.set_index('name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sick_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_table.to_csv('sick_results.csv')\n",
    "sick_table[['mixed_f1', 'mixed_f1_ci_b', 'mixed_f1_ci_t']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_table.to_csv('mult_results.csv')\n",
    "mult_table[['aupr', 'aupr_ci_b', 'aupr_ci_t']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the final Sick and Multiple classifiers -- Sick LR Silver and Mult LR Pipelined Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_sick_lr_silver, 'final_yelp_sick_model.gz')\n",
    "joblib.dump(sick_only_mult_lr_silver, 'final_yelp_mult_model.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
