{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Bias Correction\n",
    "\n",
    "Let $T(x)$ be the biased black-box selection process for labelling data at DOHMH.\n",
    "\n",
    "Let $U$ be the set of all Yelp Reviews that have been processed by the system.\n",
    "\n",
    "Let $B \\subset U$ s.t. $B = \\{(x,y) | T(x) = 1\\}$ and $B^c \\subseteq U \\setminus B$\n",
    "\n",
    "### Training: Error Rate\n",
    "\n",
    "We can model the error rate of some classifier $f$ as:\n",
    "\n",
    "$p(f(x) \\neq y) = \\Sigma_{t}{p( f(x) \\neq y | T(x) = t)p(T(x)=t)}$\n",
    "\n",
    "and use plugin estimates:\n",
    "\n",
    "$\\hat{p}(T(x) = 1) = \\frac{|B|}{|U|}$ (the % of labeled points)\n",
    "\n",
    "$\\hat{p}(f(x) \\neq y | T(x)=1) = \\frac{1}{|B|}\\Sigma_{(x,y) \\in B}{I[f(x) \\neq y]}$\n",
    "\n",
    "likewise,\n",
    "\n",
    "$\\hat{p}(f(x) \\neq y | T(x)=0) = \\frac{1}{|B^c|}\\Sigma_{(x,y) \\in B^c}{I[f(x) \\neq y]}$\n",
    "\n",
    "therefore\n",
    "\n",
    "$\\hat{p}(f(x) \\neq y) = w_B\\Sigma_{(x,y) \\in B}{I[f(x) \\neq y]} + w_{B^c}\\Sigma_{(x,y) \\in B^c}{I[f(x) \\neq y]}$\n",
    "\n",
    "where \n",
    "\n",
    "$w_B = \\frac{1}{|U|}$ and $w_{B^c} = (1 - \\frac{|B|}{|U|})(\\frac{1}{|B^c|})$\n",
    "\n",
    "Note that when $B^c$ is the entire complement of $B$, then $w_{B^c}$ reduces to $\\frac{1}{|U|}$\n",
    "\n",
    "So, if we are trying to minimize the classification error in the objective function (which cross entropy does) then these are the sample weights to be used when calculating the error\n",
    "\n",
    "### Testing: Area Under Precision-Recall Curve\n",
    "\n",
    "We will evaluate models using the AUPR metric, calculated empirically on the dev and test sets.\n",
    "\n",
    "This metric is appropriate because of the major positive-class imbalance in the dataset (loosely estimated at .5%).\n",
    "\n",
    "The tricky thing is that our measures of precision and recall need to also be importance weighted, and the importance weights for precision will vary as we vary the threshold.\n",
    "\n",
    "To see why, consider the probabilistic form of precision for some classifier $f$:\n",
    "\n",
    "#### Precision\n",
    "\n",
    "$Precision = p(y=1|f(x)=1) = \\Sigma_{t}{p(y=1 | f(x)=1, T(x) = t)p(T(x)=t|f(x)=1)}$\n",
    "\n",
    "Let $U_P \\subset U$ be the set $\\{(x,y) | f(x) = 1\\}$. \n",
    "\n",
    "**Note** that $U_P$ varies with the classification threshold.\n",
    "\n",
    "Then as with the error rate, we can find plugin estimates for the necessary quanities.\n",
    "\n",
    "$\\hat{p}(T(x) = 1|f(x)=1) = \\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B}{I[f(x)=1]} = \\frac{|B \\cap U_P|}{|U_P|}$\n",
    "\n",
    "and so $\\hat{p}(T(x) = 0|f(x)=1) = 1 - \\frac{|B \\cap U_P|}{|U_P|}$\n",
    "\n",
    "Also,\n",
    "\n",
    "$\\hat{p}(y=1|f(x)=1, T(x)=1) = \\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B\\cap U_P}{I[y=1]} \n",
    " $\n",
    "\n",
    "and $\\hat{p}(y=1|f(x)=1, T(x)=0) = \\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B^c\\cap U_P}{I[y=1]}$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$Precision = \\frac{|B \\cap U_P|}{|U_P|} \\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B\\cap U_P}{I[y=1]} \n",
    "+ (1 - \\frac{|B \\cap U_P|}{|U_P|})\\frac{1}{|U_P|} \\Sigma_{(x,y) \\in B^c\\cap U_P}{I[y=1]}$\n",
    "\n",
    "#### Recall\n",
    "\n",
    "The derivation for IW recall follows similary to IW precision.\n",
    "\n",
    "$Recall = p(f(x)=1|y=1) = \\Sigma_{t}{p(f(x)=1| y=1, T(x) = t)p(T(x)=t|y=1)}$\n",
    "\n",
    "Let $U_R \\subset U$ be the set $\\{(x,y) | y = 1\\}$. \n",
    "\n",
    "**Note** that $U_R$ **does not** vary with the classification threshold.\n",
    "\n",
    "The plugin estimates are:\n",
    "\n",
    "$\\hat{p}(T(x) = 1|y=1) = \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B}{I[y=1]} = \\frac{|B \\cap U_R|}{|U_R|}$\n",
    "\n",
    "and so $\\hat{p}(T(x) = 0|y=1) = 1 - \\frac{|B \\cap U_R|}{|U_R|}$\n",
    "\n",
    "Also,\n",
    "\n",
    "$\\hat{p}(f(x)=1 |y=1, T(x)=1) = \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B\\cap U_R}{I[f(x)=1]}$\n",
    "\n",
    "and $\\hat{p}(f(x)=1|y=1, T(x)=0) = \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B^c\\cap U_R}{I[f(x)=1]}$\n",
    "\n",
    "plugging in we have\n",
    "\n",
    "$ Recall =  \\frac{|B \\cap U_R|}{|U_R|} \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B\\cap U_R}{I[f(x)=1]}\n",
    "+ (1 - \\frac{|B \\cap U_R|}{|U_R|}) \\frac{1}{|U_R|} \\Sigma_{(x,y) \\in B^c\\cap U_R}{I[f(x)=1]}$\n",
    "\n",
    "#### AUPR Curve\n",
    "\n",
    "Finally, using the above plugin estimates, we can balance data from both $B$ and (a sample from) $B^c$.  We can obtain a series of ordered Precision-Recall points $E = \\{(p,r,t)_i | r_i \\leq r_{i'}\\}$ by varying the classification threshold $t \\in [0,1]$ and then using the trapezoidal rule to approximate the area under the Recall vs. Precision curve.\n",
    "\n",
    "#### Bootstrap\n",
    "\n",
    "For the final evaluation, we would like confidence intervals about the AUPR. The typical bootstrap method must be treated with some care.  We will have a test dataset of about 1975 biased labeled data and 1000 labeled data from it's complement. So our importance weights will be 1975/2975 and 1000/2975.  Then to calculate a bootstrap we will resample the subsets with replacement first, then calculated the IW AUPRs on the resulting sets.  This way the importance weights remain the same for each resampled dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* [X] Take the historical unbiased data and check if any of it is in the biased data. Replace all that are.\n",
    "* [X] Setup datasets from the two training regimes\n",
    "* [X] Preprocess datasets for the baselines (scikit models)\n",
    "* [X] Code up IW objective function for baselines\n",
    "* [X] Quick test training of baselines\n",
    "* [X] Code up the dev AUPR evaluation metric.\n",
    "* [ ] Batch out 100 random search cross-validated hyperparam experiments for the RF, SVM, and LR baselines\n",
    "* [ ] Implement the CNN\n",
    "* [ ] Batch out 100 random search early-stopping hyperparam experiments for CNN\n",
    "* [ ] Finish up the viz for chainer monitor\n",
    "* [ ] Implement the DyCNN\n",
    "* [ ] Redraft paper\n",
    "* [ ] Get official test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
