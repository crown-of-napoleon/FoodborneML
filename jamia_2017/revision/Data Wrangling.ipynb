{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012_before_coding.zip\r\n",
      "Before coding.zip\r\n",
      "Classified_Reviews_Tom.xlsx\r\n",
      "Classified_Reviews_Tom_2012_2016.txt\r\n",
      "Classified_Reviews_Tom_2012_2016_noformat.txt\r\n",
      "Reviews_12_11_2016.csv\r\n",
      "Reviews_2012_2013_Before_Coding.zip\r\n",
      "Reviews_4_30_2016.csv\r\n",
      "Reviews_6_16_2017.csv\r\n",
      "Unbiased_Columbia_Yelp_Reviews_Labeled_3_3_17.csv\r\n",
      "\u001b[34mbefore coding\u001b[m\u001b[m/\r\n",
      "\u001b[34mbefore coding 2\u001b[m\u001b[m/\r\n",
      "\u001b[34mbefore coding 3\u001b[m\u001b[m/\r\n",
      "biased.csv\r\n",
      "unbiased.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "sb.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data \n",
    "\n",
    "* 2012_before_coding.zip: folder containing all of the spreadsheets for reviews for which the classifier was run on in 2012\n",
    "* Reviews_2012_2013_before_coding.zip: same as above, but covers 2013 as well\n",
    "* Before_coding.zip: same as above, covering 2013-October 2015\n",
    "* Reviews_4_30_2016: reviews for which the classifier was run on for October 2015-April 2016\n",
    "* Reviews_12_11_2016: same as above, but through May 2016-12/11/2016\n",
    "* Reviews_6_16_2017: same as above, but through today and including annotations\n",
    "* For all reviews 12/11/2016 or before, the annotations are stored in Classified_Reviews_Tom.xslx (a txt file, I hope this is okay â€“ let me know if not!)\n",
    "\n",
    "### Plan\n",
    "\n",
    "* We will load all reviews with annotations into one csv (biased)\n",
    "* All others will be loaded into the unbiased dataset\n",
    "\n",
    "### There are a couple of exceptions\n",
    "\n",
    "* We drop all reviews that aren't Yes/No for the `is_foodborne` and `is_multiple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_cols = ['url', 'date', 'text', 'is_foodborne', 'is_multiple']\n",
    "unlabeled_cols = ['url', 'date', 'text']\n",
    "biased = pd.DataFrame(columns=labeled_cols)\n",
    "unbiased = pd.DataFrame(columns=unlabeled_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "older_classified = pd.read_excel('data/Classified_Reviews_Tom.xlsx', )\n",
    "older_classified.rename(columns = {'URL':'url', 'Date_Review':'date', 'Review':'text', \n",
    "                                   'Foodborne_Disease':'is_foodborne', '2_Or_More_Ill':'is_multiple'}, \n",
    "                        inplace = True)\n",
    "older_classified['date'] = pd.to_datetime(older_classified['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14275 total rows\n",
      "11308 after removal\n"
     ]
    }
   ],
   "source": [
    "# remove bad rows\n",
    "good_values = {'Yes', 'No'}\n",
    "print '{} total rows'.format(len(older_classified))\n",
    "older_classified = older_classified[older_classified['is_foodborne'].isin(good_values)]\n",
    "older_classified = older_classified[older_classified['is_multiple'].isin(good_values)]\n",
    "print '{} after removal'.format(len(older_classified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biased = biased.append(older_classified[labeled_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412829 total rows\n",
      "2251 labeled after removal\n",
      "410539 unlabeled after removal\n"
     ]
    }
   ],
   "source": [
    "newer = pd.read_csv('data/Reviews_6_16_2017.csv')\n",
    "newer.rename(columns={'multiple':'is_multiple', 'date_review':'date', 'review':'text'}, \n",
    "             inplace=True)\n",
    "newer['date'] = pd.to_datetime(newer['date'], format='%d%b%Y:%H:%M:%S.000')\n",
    "newer['is_foodborne'] = newer['is_foodborne'].map({'YES':'Yes', 'NO':'No'})\n",
    "newer['is_multiple'] = newer['is_multiple'].map({'YES':'Yes', 'NO':'No'})\n",
    "\n",
    "# remove bad rows\n",
    "good_values = {'Yes', 'No'}\n",
    "print '{} total rows'.format(len(newer))\n",
    "newer_classified = newer[newer['is_foodborne'].isin(good_values) & newer['is_multiple'].isin(good_values)]\n",
    "newer_unclassified = newer[newer['is_foodborne'].isnull()]\n",
    "# I check, all null `is_foodborne`s are null `is_multiple`s also\n",
    "print '{} labeled after removal'.format(len(newer_classified))\n",
    "print '{} unlabeled after removal'.format(len(newer_unclassified))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biased = biased.append(newer_classified[labeled_cols])\n",
    "unbiased = unbiased.append(newer_unclassified[unlabeled_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the rest of the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441280 rows before\n",
      "377758 rows after\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv('data/Reviews_4_30_2016.csv')\n",
    "csv.rename(columns={'date_review':'date', 'review':'text'}, \n",
    "           inplace=True)\n",
    "# drop bad rows and parse dates\n",
    "print '{} rows before'.format(len(csv))\n",
    "csv = csv[~csv['zip'].isnull()]\n",
    "print '{} rows after'.format(len(csv))\n",
    "csv['date'] = pd.to_datetime(csv['date'], format='%d%b%Y:%H:%M:%S.000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unbiased = unbiased.append(csv[unlabeled_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498138 rows before\n",
      "415148 rows after\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv('data/Reviews_12_11_2016.csv')\n",
    "csv.rename(columns={'date_review':'date', 'review':'text'}, \n",
    "           inplace=True)\n",
    "# drop bad rows and parse dates\n",
    "print '{} rows before'.format(len(csv))\n",
    "csv = csv[~csv['zip'].isnull()]\n",
    "print '{} rows after'.format(len(csv))\n",
    "csv['date'] = pd.to_datetime(csv['date'], format='%d%b%Y:%H:%M:%S.000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203445 total reviews\n"
     ]
    }
   ],
   "source": [
    "unbiased = unbiased.append(csv[unlabeled_cols])\n",
    "print '{} total reviews'.format(len(unbiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259 rows before 259 rows after\n",
      "61 rows before 61 rows after\n",
      "61 rows before 61 rows after\n",
      "266 rows before 266 rows after\n",
      "23 rows before 23 rows after\n",
      "2 rows before 2 rows after\n",
      "13 rows before 13 rows after\n",
      "6 rows before 6 rows after\n",
      "3 rows before 3 rows after\n",
      "32 rows before 32 rows after\n",
      "8663 rows before 8663 rows after\n",
      "8492 rows before 8492 rows after\n",
      "19 rows before 17 rows after\n",
      "14 rows before 14 rows after\n",
      "20 rows before 20 rows after\n",
      "17 rows before 17 rows after\n",
      "17 rows before 17 rows after\n",
      "36 rows before 36 rows after\n",
      "18 rows before 18 rows after\n",
      "16 rows before 16 rows after\n",
      "7 rows before 7 rows after\n",
      "7 rows before 7 rows after\n",
      "19 rows before 19 rows after\n",
      "ERROR: yelp_analysis-2012-10-10-092629.csv\n",
      "20 rows before 20 rows after\n",
      "29 rows before 29 rows after\n",
      "13 rows before 13 rows after\n",
      "4 rows before 4 rows after\n",
      "11 rows before 11 rows after\n",
      "63 rows before 63 rows after\n",
      "42 rows before 42 rows after\n",
      "33 rows before 33 rows after\n",
      "56 rows before 56 rows after\n",
      "42 rows before 42 rows after\n",
      "52 rows before 52 rows after\n",
      "39 rows before 39 rows after\n",
      "46 rows before 46 rows after\n",
      "46 rows before 46 rows after\n",
      "25 rows before 25 rows after\n",
      "29 rows before 29 rows after\n",
      "51 rows before 51 rows after\n",
      "40 rows before 40 rows after\n",
      "41 rows before 41 rows after\n",
      "78 rows before 78 rows after\n",
      "20 rows before 20 rows after\n",
      "83 rows before 83 rows after\n",
      "110 rows before 110 rows after\n",
      "82 rows before 82 rows after\n",
      "189 rows before 189 rows after\n",
      "7 rows before 7 rows after\n",
      "93 rows before 93 rows after\n",
      "53 rows before 53 rows after\n",
      "139 rows before 139 rows after\n",
      "36 rows before 36 rows after\n",
      "61 rows before 61 rows after\n",
      "51 rows before 51 rows after\n",
      "259 rows before 259 rows after\n",
      "61 rows before 61 rows after\n",
      "61 rows before 61 rows after\n",
      "266 rows before 266 rows after\n",
      "23 rows before 23 rows after\n",
      "2 rows before 2 rows after\n",
      "13 rows before 13 rows after\n",
      "6 rows before 6 rows after\n",
      "3 rows before 3 rows after\n",
      "32 rows before 32 rows after\n",
      "8663 rows before 8663 rows after\n",
      "8492 rows before 8492 rows after\n",
      "19 rows before 17 rows after\n",
      "14 rows before 14 rows after\n",
      "20 rows before 20 rows after\n",
      "17 rows before 17 rows after\n",
      "17 rows before 17 rows after\n",
      "36 rows before 36 rows after\n",
      "18 rows before 18 rows after\n",
      "16 rows before 16 rows after\n",
      "7 rows before 7 rows after\n",
      "7 rows before 7 rows after\n",
      "19 rows before 19 rows after\n",
      "ERROR: yelp_analysis-2012-10-10-092629.csv\n",
      "20 rows before 20 rows after\n",
      "29 rows before 29 rows after\n",
      "13 rows before 13 rows after\n",
      "4 rows before 4 rows after\n",
      "11 rows before 11 rows after\n",
      "259 rows before 259 rows after\n",
      "61 rows before 61 rows after\n",
      "61 rows before 61 rows after\n",
      "266 rows before 266 rows after\n",
      "23 rows before 23 rows after\n",
      "2 rows before 2 rows after\n",
      "13 rows before 13 rows after\n",
      "6 rows before 6 rows after\n",
      "3 rows before 3 rows after\n",
      "32 rows before 32 rows after\n",
      "8663 rows before 8663 rows after\n",
      "8492 rows before 8492 rows after\n",
      "19 rows before 17 rows after\n",
      "14 rows before 14 rows after\n",
      "20 rows before 20 rows after\n",
      "17 rows before 17 rows after\n",
      "17 rows before 17 rows after\n",
      "36 rows before 36 rows after\n",
      "18 rows before 18 rows after\n",
      "16 rows before 16 rows after\n",
      "7 rows before 7 rows after\n",
      "7 rows before 7 rows after\n",
      "19 rows before 19 rows after\n",
      "ERROR: yelp_analysis-2012-10-10-092629.csv\n",
      "20 rows before 20 rows after\n",
      "29 rows before 29 rows after\n",
      "13 rows before 13 rows after\n",
      "4 rows before 4 rows after\n",
      "11 rows before 11 rows after\n",
      "63 rows before 63 rows after\n",
      "42 rows before 42 rows after\n",
      "33 rows before 33 rows after\n",
      "56 rows before 56 rows after\n",
      "42 rows before 42 rows after\n",
      "52 rows before 52 rows after\n",
      "39 rows before 39 rows after\n",
      "46 rows before 46 rows after\n",
      "46 rows before 46 rows after\n",
      "25 rows before 25 rows after\n",
      "29 rows before 29 rows after\n",
      "51 rows before 51 rows after\n",
      "40 rows before 40 rows after\n",
      "41 rows before 41 rows after\n",
      "78 rows before 78 rows after\n",
      "20 rows before 20 rows after\n",
      "83 rows before 83 rows after\n",
      "110 rows before 110 rows after\n",
      "82 rows before 82 rows after\n",
      "189 rows before 189 rows after\n",
      "7 rows before 7 rows after\n",
      "93 rows before 93 rows after\n",
      "53 rows before 53 rows after\n",
      "139 rows before 139 rows after\n",
      "36 rows before 36 rows after\n",
      "61 rows before 61 rows after\n",
      "51 rows before 51 rows after\n"
     ]
    }
   ],
   "source": [
    "for d in ['before coding', 'before coding 2', 'before coding 3']:\n",
    "    for f in os.listdir('data/'+d):\n",
    "#         print(f)\n",
    "        if 'xls' in f:\n",
    "            csv = pd.read_excel('data/'+d+'/'+f)\n",
    "        else:\n",
    "            try:\n",
    "                csv = pd.read_csv('data/'+d+'/'+f)\n",
    "            except:\n",
    "                print('ERROR: {}'.format(f))\n",
    "                continue\n",
    "            \n",
    "        csv.rename(columns={'Date':'date', 'Review':'text', 'URL':'url'}, \n",
    "                   inplace=True)\n",
    "        # drop bad rows and parse dates\n",
    "#         print f\n",
    "#         print csv.head(1)\n",
    "        print '{} rows before'.format(len(csv)),\n",
    "        csv = csv[~csv['Score'].isnull()]\n",
    "        print '{} rows after'.format(len(csv))\n",
    "        csv['date'] = pd.to_datetime(csv['date'])\n",
    "        unbiased = unbiased.append(csv[unlabeled_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the final dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix up an encoding mismatch\n",
    "biased['text'] = biased['text'].map(lambda x:x.decode('ISO-8859-1') if type(x) is str else x)\n",
    "unbiased['text'] = unbiased['text'].map(lambda x:x.decode('ISO-8859-1') if type(x) is str else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13559 biased before dupe removal\n",
      "13541 biased after dupe removal\n",
      "1261097 unbiased before null removal\n",
      "1217243 unbiased after null removal\n",
      "1178842 unbiased after dupe removal\n"
     ]
    }
   ],
   "source": [
    "# remove all reviews that are not labeled from the unbiased or some unbiased nulls\n",
    "print '{} biased before dupe removal'.format(len(biased))\n",
    "biased = biased.drop_duplicates()\n",
    "print '{} biased after dupe removal'.format(len(biased))\n",
    "\n",
    "print '{} unbiased before null removal'.format(len(unbiased))\n",
    "unbiased = unbiased[~unbiased['text'].isnull()]\n",
    "print '{} unbiased after null removal'.format(len(unbiased))\n",
    "unbiased = unbiased.drop_duplicates()\n",
    "print '{} unbiased after dupe removal'.format(len(unbiased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178842 unbiased before removing biased\n"
     ]
    }
   ],
   "source": [
    "print '{} unbiased before removing biased'.format(len(unbiased))\n",
    "biased_set = { tuple(t) for t in biased[unlabeled_cols].values.tolist() }\n",
    "in_biased = unbiased.apply(lambda row:tuple(row.values) in biased_set, axis=1)\n",
    "unbiased = unbiased[~in_biased]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1176028 unbiased after removing biased\n"
     ]
    }
   ],
   "source": [
    "print '{} unbiased after removing biased'.format(len(unbiased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the final csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biased.to_csv('data/biased.csv', header=True, index=False, encoding=\"utf8\")\n",
    "unbiased.to_csv('data/unbiased.csv', header=True, index=False, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done. Final example counts:\n",
      "13541 labeled reviews\n",
      "1176028 unlabeled reviews\n"
     ]
    }
   ],
   "source": [
    "print 'All done. Final example counts:'\n",
    "print '{} labeled reviews'.format(len(biased))\n",
    "print '{} unlabeled reviews'.format(len(unbiased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also create a sample csv for labeling from the unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_date = datetime.datetime.strptime('1/1/2017', '%d/%m/%Y')\n",
    "new_unbiased = unbiased[unbiased.date >= split_date]\n",
    "sample = new_unbiased.sample(1000)\n",
    "sample['is_foodborne'] = np.nan\n",
    "sample['is_multiple'] = np.nan\n",
    "sample.to_excel('data/new_unbiased_sample_to_label.xlsx', index=False, header=True, encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
