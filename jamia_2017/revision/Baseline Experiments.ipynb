{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "sb.set()\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "biased = pd.read_csv('data/biased.csv', encoding='utf8')\n",
    "biased.date = pd.to_datetime(biased.date)\n",
    "unbiased = pd.read_csv('data/unbiased.csv', encoding='utf8', parse_dates=True)\n",
    "unbiased.date = pd.to_datetime(unbiased.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split_date = datetime.strptime('1/1/2017', '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_biased = biased[biased.date >= split_date]\n",
    "old_biased = biased[biased.date < split_date]\n",
    "new_unbiased = unbiased[unbiased.date >= split_date]\n",
    "old_unbiased = unbiased[unbiased.date < split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Is Foodborne Yes: 949 / 1975 = 48.05%\n",
      "Historical Is Foodborne Yes: 5899 / 11566 = 51.00%\n"
     ]
    }
   ],
   "source": [
    "counts = new_biased.is_foodborne.value_counts()\n",
    "print 'Current Is Foodborne Yes: {0} / {1} = {2:2.2f}%'.format(counts.Yes, counts.sum(), 100.*counts.Yes/counts.sum())\n",
    "counts = old_biased.is_foodborne.value_counts()\n",
    "print 'Historical Is Foodborne Yes: {0} / {1} = {2:2.2f}%'.format(counts.Yes, counts.sum(), 100.*counts.Yes/counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Is Multiple Yes: 277 / 1975 = 14.03%\n",
      "Historical Is Multiple Yes: 1488 / 11566 = 12.87%\n"
     ]
    }
   ],
   "source": [
    "counts = new_biased.is_multiple.value_counts()\n",
    "print 'Current Is Multiple Yes: {0} / {1} = {2:2.2f}%'.format(counts.Yes, counts.sum(), 100.*counts.Yes/counts.sum())\n",
    "counts = old_biased.is_multiple.value_counts()\n",
    "print 'Historical Is Multiple Yes: {0} / {1} = {2:2.2f}%'.format(counts.Yes, counts.sum(), 100.*counts.Yes/counts.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_baseline_data(train_regime='gold', test_regime='gold', random_seed=0, silver_size=10000):\n",
    "    test_split_date = datetime.strptime('1/1/2017', '%d/%m/%Y')\n",
    "    biased = pd.read_csv('data/biased.csv', encoding='utf8')\n",
    "    biased.date = pd.to_datetime(biased.date)\n",
    "    old_biased = biased[biased['date'] < test_split_date]\n",
    "    new_biased = biased[biased['date'] >= test_split_date]\n",
    "    \n",
    "    unbiased = pd.read_csv('data/unbiased.csv', encoding='utf8')\n",
    "    unbiased.date = pd.to_datetime(unbiased.date)\n",
    "    U = len(unbiased) + len(biased)\n",
    "    \n",
    "    if train_regime == 'gold':\n",
    "        old_unbiased = pd.read_excel('data/historical_unbiased.xlsx', encoding='utf8')\n",
    "        old_unbiased.date = pd.to_datetime(old_unbiased.date)\n",
    "    elif train_regime == 'silver':\n",
    "        old_unbiased = unbiased[unbiased.date < test_split_date]\n",
    "        old_unbiased = old_unbiased.sample(silver_size, random_state=random_seed)\n",
    "        # assume the biased complement are negative examples\n",
    "        old_unbiased['is_foodborne'] = 'No'\n",
    "        old_unbiased['is_multiple'] = 'No'\n",
    "    else:\n",
    "        raise ValueError, \"Regime must be 'silver' or 'gold'\"\n",
    "        \n",
    "    if test_regime == 'gold':\n",
    "        print '[WARNING] THE TEST DATA IS NOT CORRECT FOR THIS REGIME YET'\n",
    "        old_unbiased = pd.read_excel('data/historical_unbiased.xlsx', encoding='utf8')\n",
    "        old_unbiased.date = pd.to_datetime(old_unbiased.date)\n",
    "        # for now we are passing dummy test data\n",
    "        new_unbiased = pd.DataFrame(old_unbiased)\n",
    "    elif test_regime == 'silver':\n",
    "        unbiased = pd.read_csv('data/unbiased.csv', encoding='utf8')\n",
    "        unbiased.date = pd.to_datetime(unbiased.date)\n",
    "        new_unbiased = unbiased[unbiased.date >= test_split_date]\n",
    "        new_unbiased = new_unbiased.sample(silver_size, random_state=random_seed)\n",
    "        # assume the biased complement are negative examples\n",
    "        new_unbiased['is_foodborne'] = 'No'\n",
    "        new_unbiased['is_multiple'] = 'No'\n",
    "    else:\n",
    "        raise ValueError, \"Regime must be 'silver' or 'gold'\"\n",
    "    \n",
    "    old_biased['is_foodborne'] = old_biased['is_foodborne'].map({'Yes':1, 'No':0})\n",
    "    old_unbiased['is_foodborne'] = old_unbiased['is_foodborne'].map({'Yes':1, 'No':0})\n",
    "    new_biased['is_foodborne'] = new_biased['is_foodborne'].map({'Yes':1, 'No':0})\n",
    "    new_unbiased['is_foodborne'] = new_unbiased['is_foodborne'].map({'Yes':1, 'No':0})\n",
    "    \n",
    "    old_biased['is_multiple'] = old_biased['is_multiple'].map({'Yes':1, 'No':0})\n",
    "    old_unbiased['is_multiple'] = old_unbiased['is_multiple'].map({'Yes':1, 'No':0})\n",
    "    new_biased['is_multiple'] = new_biased['is_multiple'].map({'Yes':1, 'No':0})\n",
    "    new_unbiased['is_multiple'] = new_unbiased['is_multiple'].map({'Yes':1, 'No':0})\n",
    "    \n",
    "    return {\n",
    "        'train_data': {\n",
    "            'text':old_biased['text'].tolist() + old_unbiased['text'].tolist(),\n",
    "            'is_foodborne':old_biased['is_foodborne'].tolist() + old_unbiased['is_foodborne'].tolist(),\n",
    "            'is_multiple':old_biased['is_multiple'].tolist() + old_unbiased['is_multiple'].tolist(),\n",
    "            'is_biased':[True]*len(old_biased) + [False]*len(old_unbiased)\n",
    "        },\n",
    "        'test_data': {\n",
    "            'text':new_biased['text'].tolist() + new_unbiased['text'].tolist(),\n",
    "            'is_foodborne':new_biased['is_foodborne'].tolist() + new_unbiased['is_foodborne'].tolist(),\n",
    "            'is_multiple':new_biased['is_multiple'].tolist() + new_unbiased['is_multiple'].tolist(),\n",
    "            'is_biased':[True]*len(new_biased) + [False]*len(new_unbiased)\n",
    "        },\n",
    "        'U':U\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomaseffland/.virtualenvs/research/lib/python2.7/site-packages/ipykernel/__main__.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/thomaseffland/.virtualenvs/research/lib/python2.7/site-packages/ipykernel/__main__.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/thomaseffland/.virtualenvs/research/lib/python2.7/site-packages/ipykernel/__main__.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/thomaseffland/.virtualenvs/research/lib/python2.7/site-packages/ipykernel/__main__.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data = setup_baseline_data(train_regime='silver', test_regime='silver', random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = data['train_data']\n",
    "test_data = data['test_data']\n",
    "U = data['U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Foodborne stats\n",
      "train data: 5899 yes, 15667 no => 27.35% yes\n",
      "test data: 949 yes, 11026 no => 7.92% yes\n",
      "Is Multiple stats\n",
      "train data: 1488 yes, 20078 no => 6.90% yes\n",
      "test data: 277 yes, 11698 no => 2.31% yes\n",
      "Bias stats\n",
      "21566 train data: 11566 biased, 10000 biased complement\n",
      "11975 test data: 1975 biased, 10000 biased complement\n"
     ]
    }
   ],
   "source": [
    "print \"Is Foodborne stats\"\n",
    "print 'train data: {0} yes, {1} no => {2:2.2f}% yes'.format(\n",
    "    sum(train_data['is_foodborne']), len(train_data['is_foodborne'])-sum(train_data['is_foodborne']), \n",
    "    100.*sum(train_data['is_foodborne'])/len(train_data['is_foodborne']))\n",
    "print 'test data: {0} yes, {1} no => {2:2.2f}% yes'.format(\n",
    "    sum(test_data['is_foodborne']), len(test_data['is_foodborne'])-sum(test_data['is_foodborne']), \n",
    "    100.*sum(test_data['is_foodborne'])/len(test_data['is_foodborne']))\n",
    "\n",
    "print \"Is Multiple stats\"\n",
    "print 'train data: {0} yes, {1} no => {2:2.2f}% yes'.format(\n",
    "    sum(train_data['is_multiple']), len(train_data['is_multiple'])-sum(train_data['is_multiple']), \n",
    "    100.*sum(train_data['is_multiple'])/len(train_data['is_multiple']))\n",
    "print 'test data: {0} yes, {1} no => {2:2.2f}% yes'.format(\n",
    "    sum(test_data['is_multiple']), len(test_data['is_multiple'])-sum(test_data['is_multiple']), \n",
    "    100.*sum(test_data['is_multiple'])/len(test_data['is_multiple']))\n",
    "\n",
    "print \"Bias stats\"\n",
    "print '{} train data: {} biased, {} biased complement'.format(\n",
    "    len(train_data['text']), sum(train_data['is_biased']), len(train_data['text'])-sum(train_data['is_biased']))\n",
    "print '{} test data: {} biased, {} biased complement'.format(\n",
    "    len(test_data['text']), sum(test_data['is_biased']), len(test_data['text'])-sum(test_data['is_biased']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_train_importance_weights(is_biased, U):\n",
    "    B = float(sum(is_biased))\n",
    "    Bc = len(is_biased) - B\n",
    "#     print 'U:{}, B:{}, Bc:{}'.format(U,B, Bc)\n",
    "    w_B = 1./U\n",
    "    w_Bc = (1.-(B/U))*(1./Bc)\n",
    "    iw = np.array([w_B if label else w_Bc for label in is_biased])\n",
    "    rescaled = (len(is_biased)/iw.sum())*iw\n",
    "#     print 'w_B:{}, w_Bc:{}'.format(*sorted(np.unique(rescaled).tolist()))\n",
    "    return rescaled\n",
    "\n",
    "def importance_weighted_precision_recall(y_trues, y_pred_probs, is_biased, threshold=.5):\n",
    "    # find the precision at this threshold\n",
    "    in_Up = y_pred_probs >= threshold # same as predictions at this threshold\n",
    "    Up = in_Up.sum().astype(np.float32) + 1e-15 # for stability when there are no positive predictions\n",
    "    biased_and_Up = is_biased & in_Up\n",
    "    unbiased_and_Up = (~is_biased) & in_Up\n",
    "    bias_rate = sum(biased_and_Up)/Up\n",
    "    bias_term = bias_rate * (1./Up) * ((y_trues == 1) & biased_and_Up).sum()\n",
    "    unbias_term = (1. - bias_rate) * (1./Up) * ((y_trues == 1) & unbiased_and_Up).sum()\n",
    "    precision = bias_term + unbias_term\n",
    "\n",
    "    # find recall at this threshold\n",
    "    in_Ur = y_trues == 1 # same as true positives\n",
    "    Ur = in_Ur.sum().astype(np.float32) + 1e-15 # for stability when there are no positive examples\n",
    "    biased_and_Ur = is_biased & in_Ur\n",
    "    unbiased_and_Ur = (~is_biased) & in_Ur\n",
    "    bias_rate = sum(biased_and_Ur)/Ur\n",
    "    bias_term = bias_rate * (1./Ur) * (in_Up & biased_and_Ur).sum() # in_Up is same as preds\n",
    "    unbias_term = (1. - bias_rate) * (1./Ur) * (in_Up & unbiased_and_Ur).sum()\n",
    "    recall = bias_term + unbias_term\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "def importance_weighted_pr_curve(y_trues, y_pred_probs, is_biased):\n",
    "    thresholds = np.linspace(1, 0, 100)\n",
    "    precisions, recalls = [], []\n",
    "    for t in thresholds:\n",
    "        p, r = importance_weighted_precision_recall(y_trues, y_pred_probs, is_biased, t)\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        if r >= 1.:\n",
    "            break\n",
    "    return np.array(precisions), np.array(recalls), thresholds[:len(precisions)]\n",
    "\n",
    "def area_under_pr_curve(precisions, recalls):\n",
    "    # calculate area under curve using trapezoidal integration\n",
    "    aupr = 0.\n",
    "    for i in range(len(precisions)-1):\n",
    "        aupr += .5 * (recalls[i+1] - recalls[i]) * (precisions[i] + precisions[i+1])\n",
    "    return aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# model components\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_model(model, xs, ys, bs, U, fit_weight_kwd, n_cv_splits, random_seed):\n",
    "    folds = StratifiedKFold(n_splits=n_cv_splits, random_state=random_seed)\n",
    "    stratify_on_these = ['{},{}'.format(y,b) for y,b in zip(ys, bs)]\n",
    "    dev_scores = []\n",
    "    for train_idx, dev_idx in folds.split(np.zeros(len(xs)), stratify_on_these):\n",
    "\n",
    "        train_text = np.array(xs)[train_idx]\n",
    "        train_ys = np.array(ys)[train_idx]\n",
    "        train_is_biased = np.array(bs)[train_idx]\n",
    "\n",
    "        dev_text = np.array(xs)[dev_idx]\n",
    "        dev_ys = np.array(ys)[dev_idx]\n",
    "        dev_is_biased = np.array(bs)[dev_idx]\n",
    "        \n",
    "        train_importance_weights = calc_train_importance_weights(train_is_biased, U)\n",
    "        model.fit(train_text, train_ys, **{fit_weight_kwd:train_importance_weights})\n",
    "        scored_devs = model.predict_proba(dev_text)[:,1]\n",
    "        dev_precisions, dev_recalls, _ = importance_weighted_pr_curve(dev_ys, scored_devs, dev_is_biased)\n",
    "        dev_aupr = area_under_pr_curve(dev_precisions, dev_recalls)\n",
    "#         print 'AUPR: {0:2.2f}'.format(dev_aupr)\n",
    "        dev_scores.append(dev_aupr)\n",
    "    return np.array(dev_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_search(model, random_hyperparams, model_fname, **score_kwds):\n",
    "    experiments = []\n",
    "    N = len(random_hyperparams.values()[0])\n",
    "    best_score = -1.0\n",
    "    best_params = {}\n",
    "    for i in range(N):\n",
    "        random_hyperparam = {k:v[i] for k,v in random_hyperparams.items()}\n",
    "        model.set_params(**random_hyperparam)\n",
    "        print 'Experiment {}/{} -------'.format(i+1, N)\n",
    "        print 'params: {}'.format(random_hyperparam)\n",
    "        experiment = {'i':i,\n",
    "                      'params':model.get_params(),\n",
    "                      'scores':score_model(model, **score_kwds)}\n",
    "        experiments.append(experiment)\n",
    "        print 'scores: {}'.format(experiments[-1]['scores'])\n",
    "        score = experiments[-1]['scores'].mean()\n",
    "        if score > best_score:\n",
    "            print 'New best: {0:2.2f}'.format(score)\n",
    "            best_score = score\n",
    "            joblib.dump(model, model_fname)\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best logistic regression for sick\n",
    "f1 = CountVectorizer(\n",
    "        input=u'content', \n",
    "        encoding=u'utf-8', \n",
    "        decode_error=u'strict', \n",
    "        strip_accents=None, \n",
    "        lowercase=True, \n",
    "        preprocessor=None, \n",
    "        tokenizer=None, \n",
    "        stop_words=None, \n",
    "        ngram_range=(1, 1), \n",
    "        analyzer=u'word', \n",
    "        max_df=.95, \n",
    "        min_df=1, \n",
    "        max_features=None, \n",
    "        vocabulary=None, \n",
    "        binary=False, \n",
    "        )\n",
    "tfidf = TfidfTransformer(norm='l2', use_idf=True)\n",
    "logreg = LogisticRegression(\n",
    "    fit_intercept=True, \n",
    "    C=100, \n",
    "    penalty='l2',\n",
    "    verbose=0\n",
    ")\n",
    "sick_logreg = Pipeline([\n",
    "        ('count', f1),\n",
    "        ('tfidf', tfidf),\n",
    "        ('logreg',logreg)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1/2 -------\n",
      "params: {'count__max_df': 0.9447775573777164, 'count__ngram_range': (1, 1), 'logreg__penalty': 'l2', 'tfidf__use_idf': True, 'tfidf__norm': None, 'logreg__C': 8}\n",
      "scores: [ 0.80601555  0.77363539  0.81816675]\n",
      "New best: 0.80\n",
      "Experiment 2/2 -------\n",
      "params: {'count__max_df': 0.98916077939641511, 'count__ngram_range': (1, 2), 'logreg__penalty': 'l1', 'tfidf__use_idf': True, 'tfidf__norm': None, 'logreg__C': 63}\n",
      "scores: [ 0.83692897  0.79933422  0.8452863 ]\n",
      "New best: 0.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logreg_hyperparam_experiments.pkl']"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2\n",
    "random_hyperparams = {\n",
    "    'count__ngram_range':[(1,n) for n in npr.randint(1,4, N)],\n",
    "    'count__max_df':npr.uniform(.9, 1.0, N),\n",
    "    'tfidf__norm':npr.choice(['l1', 'l2', None], N),\n",
    "    'tfidf__use_idf':npr.choice([True, False], N),\n",
    "    'logreg__C':npr.randint(0,200, N),\n",
    "    'logreg__penalty':npr.choice(['l1', 'l2'], N)\n",
    "}\n",
    "\n",
    "score_kwds = {\n",
    "    'xs':train_data['text'],\n",
    "    'ys':train_data['is_foodborne'],\n",
    "    'bs':train_data['is_biased'],\n",
    "    'U':U,\n",
    "    'fit_weight_kwd':'logreg__sample_weight',\n",
    "    'n_cv_splits':3,\n",
    "    'random_seed':random_seed\n",
    "}\n",
    "experiments = random_search(sick_logreg, random_hyperparams, 'best_logreg.pkl', **score_kwds)\n",
    "joblib.dump(experiments, 'logreg_hyperparam_experiments.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = sorted(experiments, key=lambda x:x['scores'].mean(), reverse=True)[0]\n",
    "print best['i'], best['params'], best['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Experiments.ipynb         best_logreg.pkl\r\n",
      "Data Wrangling.ipynb               \u001b[34mdata\u001b[m\u001b[m/\r\n",
      "Evaluation.ipynb                   logreg_hyperparam_experiments.pkl\r\n",
      "IFS.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting baseline_experiment_util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile baseline_experiment_util.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def setup_baseline_data(train_regime='gold', test_regime='gold', random_seed=0, silver_size=10000):\n",
    "    test_split_date = datetime.strptime('1/1/2017', '%d/%m/%Y')\n",
    "    biased = pd.read_csv('data/biased.csv', encoding='utf8')\n",
    "    biased.date = pd.to_datetime(biased.date)\n",
    "    old_biased = biased[biased['date'] < test_split_date]\n",
    "    new_biased = biased[biased['date'] >= test_split_date]\n",
    "    \n",
    "    unbiased = pd.read_csv('data/unbiased.csv', encoding='utf8')\n",
    "    unbiased.date = pd.to_datetime(unbiased.date)\n",
    "    U = len(unbiased) + len(biased)\n",
    "    \n",
    "    if train_regime == 'gold':\n",
    "        old_unbiased = pd.read_excel('data/historical_unbiased.xlsx', encoding='utf8')\n",
    "        old_unbiased.date = pd.to_datetime(old_unbiased.date)\n",
    "    elif train_regime == 'silver':\n",
    "        old_unbiased = unbiased[unbiased.date < test_split_date]\n",
    "        old_unbiased = old_unbiased.sample(silver_size, random_state=random_seed)\n",
    "        # assume the biased complement are negative examples\n",
    "        old_unbiased['is_foodborne'] = 'No'\n",
    "        old_unbiased['is_multiple'] = 'No'\n",
    "    else:\n",
    "        raise ValueError, \"Regime must be 'silver' or 'gold'\"\n",
    "        \n",
    "    if test_regime == 'gold':\n",
    "        print '[WARNING] THE TEST DATA IS NOT CORRECT FOR THIS REGIME YET'\n",
    "        old_unbiased = pd.read_excel('data/historical_unbiased.xlsx', encoding='utf8')\n",
    "        old_unbiased.date = pd.to_datetime(old_unbiased.date)\n",
    "        # for now we are passing dummy test data\n",
    "        new_unbiased = pd.DataFrame(old_unbiased)\n",
    "    elif test_regime == 'silver':\n",
    "        unbiased = pd.read_csv('data/unbiased.csv', encoding='utf8')\n",
    "        unbiased.date = pd.to_datetime(unbiased.date)\n",
    "        new_unbiased = unbiased[unbiased.date >= test_split_date]\n",
    "        new_unbiased = new_unbiased.sample(silver_size, random_state=random_seed)\n",
    "        # assume the biased complement are negative examples\n",
    "        new_unbiased['is_foodborne'] = 'No'\n",
    "        new_unbiased['is_multiple'] = 'No'\n",
    "    else:\n",
    "        raise ValueError, \"Regime must be 'silver' or 'gold'\"\n",
    "    \n",
    "    old_biased['is_foodborne'] = old_biased['is_foodborne'].map({'Yes':1, 'No':0})\n",
    "    old_unbiased['is_foodborne'] = old_unbiased['is_foodborne'].map({'Yes':1, 'No':0})\n",
    "    new_biased['is_foodborne'] = new_biased['is_foodborne'].map({'Yes':1, 'No':0})\n",
    "    new_unbiased['is_foodborne'] = new_unbiased['is_foodborne'].map({'Yes':1, 'No':0})\n",
    "    \n",
    "    old_biased['is_multiple'] = old_biased['is_multiple'].map({'Yes':1, 'No':0})\n",
    "    old_unbiased['is_multiple'] = old_unbiased['is_multiple'].map({'Yes':1, 'No':0})\n",
    "    new_biased['is_multiple'] = new_biased['is_multiple'].map({'Yes':1, 'No':0})\n",
    "    new_unbiased['is_multiple'] = new_unbiased['is_multiple'].map({'Yes':1, 'No':0})\n",
    "    \n",
    "    return {\n",
    "        'train_data': {\n",
    "            'text':old_biased['text'].tolist() + old_unbiased['text'].tolist(),\n",
    "            'is_foodborne':old_biased['is_foodborne'].tolist() + old_unbiased['is_foodborne'].tolist(),\n",
    "            'is_multiple':old_biased['is_multiple'].tolist() + old_unbiased['is_multiple'].tolist(),\n",
    "            'is_biased':[True]*len(old_biased) + [False]*len(old_unbiased)\n",
    "        },\n",
    "        'test_data': {\n",
    "            'text':new_biased['text'].tolist() + new_unbiased['text'].tolist(),\n",
    "            'is_foodborne':new_biased['is_foodborne'].tolist() + new_unbiased['is_foodborne'].tolist(),\n",
    "            'is_multiple':new_biased['is_multiple'].tolist() + new_unbiased['is_multiple'].tolist(),\n",
    "            'is_biased':[True]*len(new_biased) + [False]*len(new_unbiased)\n",
    "        },\n",
    "        'U':U\n",
    "    }\n",
    "\n",
    "def calc_train_importance_weights(is_biased, U):\n",
    "    B = float(sum(is_biased))\n",
    "    Bc = len(is_biased) - B\n",
    "#     print 'U:{}, B:{}, Bc:{}'.format(U,B, Bc)\n",
    "    w_B = 1./U\n",
    "    w_Bc = (1.-(B/U))*(1./Bc)\n",
    "    iw = np.array([w_B if label else w_Bc for label in is_biased])\n",
    "    rescaled = (len(is_biased)/iw.sum())*iw\n",
    "#     print 'w_B:{}, w_Bc:{}'.format(*sorted(np.unique(rescaled).tolist()))\n",
    "    return rescaled\n",
    "\n",
    "def importance_weighted_precision_recall(y_trues, y_pred_probs, is_biased, threshold=.5):\n",
    "    # find the precision at this threshold\n",
    "    in_Up = y_pred_probs >= threshold # same as predictions at this threshold\n",
    "    Up = in_Up.sum().astype(np.float32) + 1e-15 # for stability when there are no positive predictions\n",
    "    biased_and_Up = is_biased & in_Up\n",
    "    unbiased_and_Up = (~is_biased) & in_Up\n",
    "    bias_rate = sum(biased_and_Up)/Up\n",
    "    bias_term = bias_rate * (1./Up) * ((y_trues == 1) & biased_and_Up).sum()\n",
    "    unbias_term = (1. - bias_rate) * (1./Up) * ((y_trues == 1) & unbiased_and_Up).sum()\n",
    "    precision = bias_term + unbias_term\n",
    "\n",
    "    # find recall at this threshold\n",
    "    in_Ur = y_trues == 1 # same as true positives\n",
    "    Ur = in_Ur.sum().astype(np.float32) + 1e-15 # for stability when there are no positive examples\n",
    "    biased_and_Ur = is_biased & in_Ur\n",
    "    unbiased_and_Ur = (~is_biased) & in_Ur\n",
    "    bias_rate = sum(biased_and_Ur)/Ur\n",
    "    bias_term = bias_rate * (1./Ur) * (in_Up & biased_and_Ur).sum() # in_Up is same as preds\n",
    "    unbias_term = (1. - bias_rate) * (1./Ur) * (in_Up & unbiased_and_Ur).sum()\n",
    "    recall = bias_term + unbias_term\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "def importance_weighted_pr_curve(y_trues, y_pred_probs, is_biased):\n",
    "    thresholds = np.linspace(1, 0, 100)\n",
    "    precisions, recalls = [], []\n",
    "    for t in thresholds:\n",
    "        p, r = importance_weighted_precision_recall(y_trues, y_pred_probs, is_biased, t)\n",
    "        precisions.append(p)\n",
    "        recalls.append(r)\n",
    "        if r >= 1.:\n",
    "            break\n",
    "    return np.array(precisions), np.array(recalls), thresholds[:len(precisions)]\n",
    "\n",
    "def area_under_pr_curve(precisions, recalls):\n",
    "    # calculate area under curve using trapezoidal integration\n",
    "    aupr = 0.\n",
    "    for i in range(len(precisions)-1):\n",
    "        aupr += .5 * (recalls[i+1] - recalls[i]) * (precisions[i] + precisions[i+1])\n",
    "    return aupr\n",
    "\n",
    "def score_model(model, xs, ys, bs, U, fit_weight_kwd, n_cv_splits, random_seed):\n",
    "    folds = StratifiedKFold(n_splits=n_cv_splits, random_state=random_seed)\n",
    "    stratify_on_these = ['{},{}'.format(y,b) for y,b in zip(ys, bs)]\n",
    "    dev_scores = []\n",
    "    for train_idx, dev_idx in folds.split(np.zeros(len(xs)), stratify_on_these):\n",
    "\n",
    "        train_text = np.array(xs)[train_idx]\n",
    "        train_ys = np.array(ys)[train_idx]\n",
    "        train_is_biased = np.array(bs)[train_idx]\n",
    "\n",
    "        dev_text = np.array(xs)[dev_idx]\n",
    "        dev_ys = np.array(ys)[dev_idx]\n",
    "        dev_is_biased = np.array(bs)[dev_idx]\n",
    "        \n",
    "        train_importance_weights = calc_train_importance_weights(train_is_biased, U)\n",
    "        model.fit(train_text, train_ys, **{fit_weight_kwd:train_importance_weights})\n",
    "        scored_devs = model.predict_proba(dev_text)[:,1]\n",
    "        dev_precisions, dev_recalls, _ = importance_weighted_pr_curve(dev_ys, scored_devs, dev_is_biased)\n",
    "        dev_aupr = area_under_pr_curve(dev_precisions, dev_recalls)\n",
    "#         print 'AUPR: {0:2.2f}'.format(dev_aupr)\n",
    "        dev_scores.append(dev_aupr)\n",
    "    return np.array(dev_scores)\n",
    "\n",
    "def random_search(model, random_hyperparams, model_fname, **score_kwds):\n",
    "    experiments = []\n",
    "    N = len(random_hyperparams.values()[0])\n",
    "    best_score = -1.0\n",
    "    best_params = {}\n",
    "    for i in range(N):\n",
    "        random_hyperparam = {k:v[i] for k,v in random_hyperparams.items()}\n",
    "        model.set_params(**random_hyperparam)\n",
    "        print '\\n------- Experiment {}/{} -------'.format(i+1, N)\n",
    "        print 'params: {}'.format(random_hyperparam)\n",
    "        experiment = {'i':i,\n",
    "                      'params':model.get_params(),\n",
    "                      'scores':score_model(model, **score_kwds)}\n",
    "        experiments.append(experiment)\n",
    "        print 'scores: {}'.format(experiments[-1]['scores'])\n",
    "        score = experiments[-1]['scores'].mean()\n",
    "        if score > best_score:\n",
    "            print 'New best: {0:2.2f}'.format(score)\n",
    "            best_score = score\n",
    "            joblib.dump(model, model_fname)\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logreg_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logreg_model.py\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#logistic regression pipeline def\n",
    "f1 = CountVectorizer(\n",
    "        input=u'content', \n",
    "        encoding=u'utf-8', \n",
    "        decode_error=u'strict', \n",
    "        strip_accents=None, \n",
    "        lowercase=True, \n",
    "        preprocessor=None, \n",
    "        tokenizer=None, \n",
    "        stop_words=None, \n",
    "        ngram_range=(1, 1), \n",
    "        analyzer=u'word', \n",
    "        max_df=.95, \n",
    "        min_df=1, \n",
    "        max_features=None, \n",
    "        vocabulary=None, \n",
    "        binary=False, \n",
    "        )\n",
    "tfidf = TfidfTransformer(norm='l2', use_idf=True)\n",
    "logreg = LogisticRegression(\n",
    "    fit_intercept=True, \n",
    "    C=100, \n",
    "    penalty='l2',\n",
    "    verbose=0\n",
    ")\n",
    "model = Pipeline([\n",
    "        ('count', f1),\n",
    "        ('tfidf', tfidf),\n",
    "        ('logreg',logreg)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting logreg_sick_dev.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile logreg_sick_dev.py\n",
    "import numpy.random as npr\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from baseline_experiment_util import setup_baseline_data, random_search\n",
    "from logreg_model import model\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "data = setup_baseline_data(train_regime='silver', test_regime='silver', random_seed=random_seed)\n",
    "train_data = data['train_data']\n",
    "U = data['U']\n",
    "\n",
    "N = 2\n",
    "random_hyperparams = {\n",
    "    'count__ngram_range':[(1,n) for n in npr.randint(1,4, N)],\n",
    "    'count__max_df':npr.uniform(.9, 1.0, N),\n",
    "    'tfidf__norm':npr.choice(['l1', 'l2', None], N),\n",
    "    'tfidf__use_idf':npr.choice([True, False], N),\n",
    "    'logreg__C':npr.randint(0,200, N),\n",
    "    'logreg__penalty':npr.choice(['l1', 'l2'], N)\n",
    "}\n",
    "\n",
    "score_kwds = {\n",
    "    'xs':train_data['text'],\n",
    "    'ys':train_data['is_foodborne'],\n",
    "    'bs':train_data['is_biased'],\n",
    "    'U':U,\n",
    "    'fit_weight_kwd':'logreg__sample_weight',\n",
    "    'n_cv_splits':3,\n",
    "    'random_seed':random_seed\n",
    "}\n",
    "experiments = random_search(model, random_hyperparams, 'best_logreg.pkl', **score_kwds)\n",
    "print 'Writing out experiments'\n",
    "joblib.dump(experiments, 'logreg_hyperparam_experiments.pkl')\n",
    "print' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thomaseffland/Development/projects/fbnyc/FoodborneNYC/jamia_2017/revision/baseline_experiment_util.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  old_biased['is_foodborne'] = old_biased['is_foodborne'].map({'Yes':1, 'No':0})\n",
      "/Users/thomaseffland/Development/projects/fbnyc/FoodborneNYC/jamia_2017/revision/baseline_experiment_util.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  new_biased['is_foodborne'] = new_biased['is_foodborne'].map({'Yes':1, 'No':0})\n",
      "/Users/thomaseffland/Development/projects/fbnyc/FoodborneNYC/jamia_2017/revision/baseline_experiment_util.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  old_biased['is_multiple'] = old_biased['is_multiple'].map({'Yes':1, 'No':0})\n",
      "/Users/thomaseffland/Development/projects/fbnyc/FoodborneNYC/jamia_2017/revision/baseline_experiment_util.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  new_biased['is_multiple'] = new_biased['is_multiple'].map({'Yes':1, 'No':0})\n",
      "Experiment 1/2 -------\n",
      "params: {'count__max_df': 0.98859654697449828, 'count__ngram_range': (1, 2), 'logreg__penalty': 'l2', 'tfidf__use_idf': True, 'tfidf__norm': 'l1', 'logreg__C': 191}\n",
      "scores: [ 0.61765305  0.59975562  0.63706569]\n",
      "New best: 0.62\n",
      "Experiment 2/2 -------\n",
      "params: {'count__max_df': 0.96708010643436237, 'count__ngram_range': (1, 1), 'logreg__penalty': 'l2', 'tfidf__use_idf': True, 'tfidf__norm': 'l2', 'logreg__C': 75}\n",
      "scores: [ 0.80685496  0.78571487  0.82533813]\n",
      "New best: 0.81\n"
     ]
    }
   ],
   "source": [
    "!python logreg_sick_dev.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
